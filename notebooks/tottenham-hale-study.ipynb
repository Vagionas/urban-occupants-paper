{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tottenham Hale Study\n",
    "\n",
    "This notebook performs a energy agents study for Tottenham Hale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import requests_cache\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "from pytus2000 import read_diary_file, diary, read_individual_file, individual, read_diary_file_as_timeseries\n",
    "import pytus2000\n",
    "import people as ppl\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import ktp.census\n",
    "import ktp.synthpop\n",
    "import ktp.tus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_FOLDER_PATH = Path('./build/')\n",
    "TUS_DATA_FOLDER_PATH = Path('./data/UKDA-4504-tab/')\n",
    "MIDAS_DATABASE_PATH = Path('./data/Londhour.csv')\n",
    "\n",
    "PATH_TO_DB = (BUILD_FOLDER_PATH / ('tottenham-hale-scenario.db')).absolute()\n",
    "PATH_TO_DB.parent.mkdir(parents=True, exist_ok=True)\n",
    "MARKOV_CHAIN_INDEX_TABLE_NAME = 'markovChains'\n",
    "DWELLINGS_TABLE_NAME = 'dwellings'\n",
    "PEOPLE_TABLE_NAME = 'people'\n",
    "ENVIRONMENT_TABLE_NAME = 'environment'\n",
    "PARAMETERS_TABLE_NAME = 'parameters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed('tottenham-hale-scenario')\n",
    "pytus2000.set_cache_location(BUILD_FOLDER_PATH)\n",
    "requests_cache.install_cache((BUILD_FOLDER_PATH / 'web-cache').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_db(df, table_name):\n",
    "    disk_engine = sqlalchemy.create_engine('sqlite:///{}'.format(PATH_TO_DB))\n",
    "    df.to_sql(name=table_name, con=disk_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress_bar(generator, progress_bar):\n",
    "    for elem in generator:\n",
    "        progress_bar.value += 1\n",
    "        yield elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, clean, and map all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_HOUSEHOLDS_HARINGEY = 101955\n",
    "NUMBER_USUAL_RESIDENTS_HARINGEY = 254926\n",
    "NUMBER_HOUSEHOLDS = 5384\n",
    "NUMBER_USUAL_RESIDENTS = 15064\n",
    "TIME_STEP_SIZE = datetime.timedelta(minutes=10)\n",
    "START_TIME = datetime.datetime(2005, 1, 1, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participants\n",
    "\n",
    "First off, let's define the group of people we are using from the UK Time Use Survey 2000 as seed for the synthetic population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_data = read_individual_file(TUS_DATA_FOLDER_PATH / 'tab' / 'individual_data_5.tab')\n",
    "## TODO filter city population\n",
    "seed = pd.DataFrame(index=individual_data.index, columns=['labour', 'qualification', 'age', 'hhtype'])\n",
    "seed.labour = individual_data.ECONACT2.map(ktp.tus.LABOUR_MAP)\n",
    "seed.labour[individual_data.IAGE > 74] = ktp.census.Labour.ABOVE_74\n",
    "seed.qualification = individual_data.HIQUAL4.map(ktp.tus.QUALIFICATION_MAP)\n",
    "seed['age'] = individual_data.IAGE.copy()\n",
    "seed['hhtype'] = individual_data.HHTYPE4.map(ktp.tus.HOUSEHOLDTYPE_MAP)\n",
    "seed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.dropna(axis='index', how='any', inplace=True)\n",
    "assert not seed.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A household is invalid if the amount of individuals we have do not match the household type. For example, a couple without children household must have exactly two individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_types = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.first()\n",
    "household_sizes = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.count()\n",
    "mask_couples_children = household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN) & (household_sizes <= 2)]\n",
    "mask_couples_no_children = household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN) & (household_sizes != 2)]\n",
    "invalids = (\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN) & (household_sizes <= 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN) & (household_sizes != 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.LONE_PARENT_WITH_DEPENDENT_CHILDREN) & (household_sizes < 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.MULTI_PERSON_HOUSEHOLD) & (household_sizes <= 2)]\n",
    ")\n",
    "\n",
    "seed.drop(labels=invalids.index, level=None, inplace=True)\n",
    "\n",
    "\n",
    "print(\"{} households are invalid and were removed.\".format(invalids.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test all input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not ((seed.labour == ktp.census.Labour.ABOVE_74) & (seed.age <= 74)).any()\n",
    "assert not ((seed.labour == ktp.census.Labour.BELOW_16) & (seed.age >= 16)).any()\n",
    "assert not ((seed.labour == ktp.census.Labour.BELOW_16) & (seed.qualification != ktp.census.Qualification.BELOW_16)).any()\n",
    "assert not ((seed.labour != ktp.census.Labour.BELOW_16) & (seed.qualification == ktp.census.Qualification.BELOW_16)).any()\n",
    "\n",
    "household_types = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.first()\n",
    "household_sizes = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.count()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.ONE_PERSON_HOUSEHOLD] == 1).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN] > 2).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.LONE_PARENT_WITH_DEPENDENT_CHILDREN] > 1).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN] == 2).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.MULTI_PERSON_HOUSEHOLD] > 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Markov Chains\n",
    "\n",
    "Now that we have the participants, we can create markov chain for each type of citizen. A type is defined by two attributes:\n",
    "\n",
    "* the current work status; 'labour' in the following\n",
    "* the highest qualification received, 'qualification' in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_data = read_diary_file(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')\n",
    "diary_data_ts = read_diary_file_as_timeseries(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')[['activity', 'location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ts = pd.DataFrame({\n",
    "    'location': diary_data_ts.location.map(ktp.tus.LOCATION_MAP),\n",
    "    'activity': diary_data_ts.activity.map(ktp.tus.ACTIVITY_MAP)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Handle Unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ts.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simple_ts[(simple_ts.activity == ktp.tus.Activity.UNKNOWN) | (simple_ts.location == ktp.tus.Location.UNKNOWN)]) / len(simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5% of all entries are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts = simple_ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.replace(to_replace=[ktp.tus.Location.UNKNOWN, ktp.tus.Activity.UNKNOWN], value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknowns will be filled by forward fill. That is, whenever  an acticity/location is unknown it is expected that the last known activity/location is still valid. \n",
    "\n",
    "When doing that, it is important to not forward fill between diaries (all diaries are below each other). Hence, they must be grouped into diaries first and then forward filled. This will lead to the fact that not all Unknowns can be filled (the ones at the beginning of the day), but that is wanted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts = filled_simple_ts.groupby([filled_simple_ts.index.get_level_values(0), \n",
    "                                             filled_simple_ts.index.get_level_values(1), \n",
    "                                             filled_simple_ts.index.get_level_values(2), \n",
    "                                             filled_simple_ts.index.get_level_values(3)]).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO don't forward fill over too long durations, e.g. not more than 1-2h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining nans are filtered in the Filter section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to markov states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = ktp.tus.from_simplified_location_and_activity_to_people_model(filled_simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nan(markov_ts, diary_data):\n",
    "    \"\"\"Remove all diaries with at least one NaN.\"\"\"\n",
    "    nan_mask = markov_ts.groupby(by=lambda index: (index[0], index[1], index[2], index[3])).apply(lambda values: values.isnull().any())\n",
    "    return pd.DataFrame(markov_ts)[markov_ts.index.droplevel(4).isin(nan_mask[~nan_mask].index)]\n",
    "\n",
    "markov_ts = filter_nan(markov_ts, diary_data)\n",
    "assert not markov_ts.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster by synth pop categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_groups = seed.groupby(['labour', 'qualification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_chain_for_group_of_people(markov_ts, group_of_people, weekdays, weekenddays):\n",
    "    # filter by people\n",
    "    people_mask = markov_ts.index.droplevel([3, 4]).isin(group_of_people.index)\n",
    "    filtered_markov = pd.DataFrame(markov_ts)[people_mask]\n",
    "    # filter by weekday\n",
    "    weekday_mask = filtered_markov.index.droplevel([4]).isin(weekdays.index)\n",
    "    filtered_markov_weekday = filtered_markov[weekday_mask]\n",
    "    # filter by weekend\n",
    "    weekend_mask = filtered_markov.index.droplevel([4]).isin(weekenddays.index)\n",
    "    filtered_markov_weekend = filtered_markov[weekend_mask]\n",
    "    return ppl.WeekMarkovChain(\n",
    "        weekday_time_series=filtered_markov_weekday.unstack(level=[0, 1, 2, 3]),\n",
    "        weekend_time_series=filtered_markov_weekend.unstack(level=[0, 1, 2, 3]),\n",
    "        time_step_size=TIME_STEP_SIZE\n",
    "    )\n",
    "\n",
    "\n",
    "def people_group(labour, qualification):\n",
    "    group = seed_groups.get_group((labour, qualification))\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = diary_data[diary_data.DDAYW2 == diary.DDAYW2.WEEKDAY_MON___FRI]\n",
    "weekenddays = diary_data[diary_data.DDAYW2 == diary.DDAYW2.WEEKEND_DAY]\n",
    "\n",
    "progress_bar = ipywidgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len([qual for qual in ktp.census.Qualification]) * len([lab for lab in ktp.census.Labour]),\n",
    "    step=1,\n",
    "    description='Progress:',\n",
    "    bar_style='',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "display(progress_bar)\n",
    "\n",
    "markov_chains = {\n",
    "    (labour, qualification): markov_chain_for_group_of_people(\n",
    "        markov_ts=markov_ts, \n",
    "        group_of_people=people_group(labour, qualification),\n",
    "        weekdays=weekdays,\n",
    "        weekenddays=weekenddays\n",
    "    ) if (labour, qualification) in seed_groups.groups.keys() else None\n",
    "    for labour in ktp.census.Labour\n",
    "    for qualification in update_progress_bar(ktp.census.Qualification, progress_bar)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amend seed by markov chain attribute\n",
    "\n",
    "Now that we have calculated all markov chains, the id associated with the markov chain will be added as an attribute to the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_id(labour, qualification):\n",
    "    # cantor pairing function, http://stackoverflow.com/a/919661/1856079\n",
    "    x = labour.value\n",
    "    y = qualification.value\n",
    "    return int(1/2 * (x + y) * (x + y + 1) + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed['markov_id'] = seed.apply(\n",
    "    lambda row: markov_id(row.labour, row.qualification), \n",
    "    axis=1\n",
    ")\n",
    "seed['initial_activity'] = seed.apply(\n",
    "    lambda row: markov_chains[(row.labour, row.qualification)].valid_states(START_TIME)[0], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fifteen_haringey = ktp.census.read_borough_population_data('Haringey').ix[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of fifteen year old is important as it is used to divide adults from youth in the census (see lateron). The ward population data set cuts based on 5 years and hence does not cut between youth and adults. For reference, the total number of fifteen year old is read in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usual_residents = ktp.census.read_ward_population_data('Haringey')\n",
    "assert usual_residents.sum().sum() == NUMBER_USUAL_RESIDENTS_HARINGEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data = ktp.census.read_households('Haringey')\n",
    "household_data = household_data[list(ktp.census.HOUSEHOLDTYPE_MAP.keys())] # pick important columns\n",
    "household_data = household_data.rename(columns=ktp.census.HOUSEHOLDTYPE_MAP).groupby(lambda x:x, axis=1).sum()\n",
    "assert household_data.sum().sum() == NUMBER_HOUSEHOLDS_HARINGEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualification_data = ktp.census.read_qualification('Haringey')\n",
    "qualification_data = qualification_data[list(ktp.census.QUALIFICATION_MAP.keys())] # pick important columns\n",
    "qualification_data = qualification_data.rename(columns=ktp.census.QUALIFICATION_MAP).groupby(lambda x:x, axis=1).sum()\n",
    "assert qualification_data.sum().sum() == usual_residents.ix[:, '15 to 19':].sum().sum() - total_fifteen_haringey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualification data is available for every usual resident starting from age 16. Unfortunately there is no data available on how many residents are older or younger than 16 (only 15) in the ward population data set. But obviously, we can infer the number of residents below 16 from the qualification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "younger_than_sixteen = usual_residents.sum(axis=1) - qualification_data.sum(axis=1)\n",
    "qualification_data[ktp.census.Qualification.BELOW_16] = younger_than_sixteen\n",
    "assert qualification_data.sum().sum() == usual_residents.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labour_data = ktp.census.read_labour_data('Haringey')\n",
    "labour_data = labour_data[list(ktp.census.LABOUR_MAP.keys())] # pick important columns\n",
    "labour_data = labour_data.rename(columns=ktp.census.LABOUR_MAP).groupby(lambda x:x, axis=1).sum()\n",
    "assert labour_data.sum().sum() == usual_residents.ix[:, '15 to 19':'70 to 74'].sum().sum() - total_fifteen_haringey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labour data is available for every usual resident between age 16 and 74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labour_data[ktp.census.Labour.BELOW_16] = younger_than_sixteen\n",
    "labour_data[ktp.census.Labour.ABOVE_74] = usual_residents.ix[:, '75 to 79':].sum(axis=1)\n",
    "assert labour_data.sum().sum() == usual_residents.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare index\n",
    "sn1_plus_sn2 = seed.index.droplevel(2)\n",
    "seed = seed.copy()\n",
    "seed['household_id'] = list(sn1_plus_sn2)\n",
    "seed.reset_index(inplace=True)\n",
    "seed.rename(columns={'SN3': 'person_id'}, inplace=True)\n",
    "seed.set_index(['household_id', 'person_id'], inplace=True)\n",
    "seed.drop(['SN1', 'SN2'], axis=1, inplace=True)\n",
    "seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the iterative proportional fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_weights = ktp.synthpop.fit_hipf(\n",
    "    reference_sample=seed,\n",
    "    controls_households={'hhtype': household_data.sum().to_dict()},\n",
    "    controls_individuals={\n",
    "        'labour': labour_data.sum().to_dict(),\n",
    "        'qualification': qualification_data.sum().to_dict()\n",
    "    },\n",
    "    residuals_tol=0.0001,\n",
    "    weights_tol=0.0001,\n",
    "    maxiter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert NUMBER_HOUSEHOLDS - household_weights.sum() < 0.1\n",
    "assert not any(household_weights.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Synthetic Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dwelling = namedtuple('Dwelling', ['id', 'householdType'])\n",
    "Citizen = namedtuple('Citizen', ['dwellingId', 'markovId', 'initialActivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_population(household_weights, seed, number_households):\n",
    "    norm_household_weights = household_weights.copy()\n",
    "    norm_household_weights = household_weights / household_weights.sum()\n",
    "    cum_norm_household_weights = norm_household_weights.cumsum()\n",
    "    assert math.isclose(norm_household_weights.sum(), 1, abs_tol=0.001)\n",
    "    dwellings = []\n",
    "    citizens = []\n",
    "    for i in range(1, number_households + 1):\n",
    "        household_id = sample_household(cum_norm_household_weights)\n",
    "        dwellings.append(Dwelling(i, seed.ix[(household_id), :].iloc[0].hhtype))\n",
    "        inhabitants = seed.ix[household_id, :]\n",
    "        for index, row in inhabitants.iterrows():\n",
    "            citizens.append(Citizen(\n",
    "                dwellingId=i, \n",
    "                markovId=row.markov_id,\n",
    "                initialActivity=row.initial_activity\n",
    "            ))\n",
    "    return dwellings, citizens\n",
    "\n",
    "def sample_household(cumulated_household_weights):\n",
    "    random_number = random.uniform(0, 1)\n",
    "    return cumulated_household_weights[cumulated_household_weights >= random_number].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwellings, citizens = create_synthetic_population(\n",
    "    household_weights=household_weights, \n",
    "    seed=seed, \n",
    "    number_households=NUMBER_HOUSEHOLDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Synthetic Population with Parameters from UKBuildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ; for the moment use random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformDistributedParameter():\n",
    "    \n",
    "    def __init__(self, expected_value, variation_in_percent):\n",
    "        self.__expected_value = expected_value\n",
    "        self.__random_max = expected_value * variation_in_percent / 100\n",
    "        \n",
    "    def sample(self):\n",
    "        return self.__expected_value + random.uniform(-self.__random_max, self.__random_max)\n",
    "   \n",
    "\n",
    "CONDITIONED_FLOOR_AREA = 100 # m^2\n",
    "HEAT_MASS_CAPACITY = UniformDistributedParameter(165000 * CONDITIONED_FLOOR_AREA, 20.0)\n",
    "HEAT_TRANSMISSION = UniformDistributedParameter(200, 20.0)\n",
    "MAX_HEATING_POWER = 10000\n",
    "MAX_COOLING_POWER = -10000\n",
    "INITIAL_TEMPERATURE = UniformDistributedParameter(22, 35.2)\n",
    "HEATING_CONTROL_STRATEGY = 'TIME_TRIGGERED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwellings_df = pd.DataFrame(\n",
    "    index=[dwelling.id for dwelling in dwellings],\n",
    "    data={\n",
    "        'heatMassCapacity': [HEAT_MASS_CAPACITY.sample() for unused in dwellings],\n",
    "        'heatTransmission': [HEAT_TRANSMISSION.sample() for unused in dwellings],\n",
    "        'maxHeatingPower': MAX_HEATING_POWER,\n",
    "        'maxCoolingPower': MAX_COOLING_POWER,\n",
    "        'initialTemperature': [INITIAL_TEMPERATURE.sample() for unused in dwellings],\n",
    "        'conditionedFloorArea': CONDITIONED_FLOOR_AREA,\n",
    "        'heatingControlStrategy': HEATING_CONTROL_STRATEGY\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citizen_df = pd.DataFrame(\n",
    "    index=list(range(len(citizens))),\n",
    "    data={\n",
    "        'markovChainId': [citizen.markovId for citizen in citizens],\n",
    "        'dwellingId': [citizen.dwellingId for citizen in citizens],\n",
    "        'initialActivity': [str(citizen.initialActivity) for citizen in citizens]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revert indices to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_weights = household_weights.reset_index().rename(columns={'household_id': 'SN1'})\n",
    "household_weights['SN2'] = [x[1] for x in household_weights['SN1']]\n",
    "household_weights['SN1'] = [x[0] for x in household_weights['SN1']]\n",
    "household_weights.set_index(['SN1', 'SN2'], inplace=True)\n",
    "household_weights = household_weights[0]\n",
    "household_weights.name = 'household weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = seed.reset_index().rename(columns={'household_id': 'SN1', 'person_id': 'SN3'})\n",
    "seed['SN2'] = [x[1] for x in seed['SN1']]\n",
    "seed['SN1'] = [x[0] for x in seed['SN1']]\n",
    "seed.set_index(['SN1', 'SN2', 'SN3'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_index = pd.Series(\n",
    "    {\n",
    "        markov_id(labour, qualification): \"markov{}\".format(markov_id(labour, qualification))\n",
    "        for labour, qualification in markov_chains.keys() if markov_chains[(labour, qualification)] is not None\n",
    "    }, \n",
    "    name='tablename'\n",
    ")\n",
    "df_to_db(markov_index, MARKOV_CHAIN_INDEX_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, markov_chain in markov_chains.items():\n",
    "    if markov_chain is None:\n",
    "        continue\n",
    "    labour, qualification = key\n",
    "    df = markov_chain.to_dataframe()\n",
    "    df.fromActivity = [str(x) for x in df.fromActivity]\n",
    "    df.toActivity = [str(x) for x in df.toActivity]\n",
    "    df_to_db(df, markov_index[markov_id(labour, qualification)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_db(dwellings_df, DWELLINGS_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_db(citizen_df, PEOPLE_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_parser(date, time):\n",
    "    month, day, year = [int(x) for x in date.split('/')]\n",
    "    hour, minute = [int(x) for x in time.split(':')]\n",
    "    return datetime.datetime(year, month, day, hour - 1, minute)\n",
    "\n",
    "temperature = pd.read_csv(\n",
    "    MIDAS_DATABASE_PATH, \n",
    "    skiprows=[0], \n",
    "    header=0, \n",
    "    parse_dates=[['Date (MM/DD/YYYY)', 'Time (HH:MM)']], \n",
    "    date_parser=date_parser,\n",
    "    index_col=[0]\n",
    ")\n",
    "temperature.rename(columns={'Dry-bulb (C)': 'temperature'}, inplace=True)\n",
    "temperature.index.name = 'index'\n",
    "df_to_db(temperature['temperature'].resample(TIME_STEP_SIZE).ffill(), ENVIRONMENT_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_db(\n",
    "    table_name=PARAMETERS_TABLE_NAME,\n",
    "    df=pd.DataFrame(\n",
    "        index=[1],\n",
    "        data={\n",
    "            'initialDatetime': START_TIME,\n",
    "            'timeStepSize_in_min': TIME_STEP_SIZE.total_seconds() / 60,\n",
    "            'numberTimeSteps': 6 * 24,\n",
    "            'randomSeed': 123456789,\n",
    "            'setPointWhileHome': 22.0,\n",
    "            'setPointWhileAsleep': 18.0,\n",
    "            'wakeUpTime': datetime.time(7, 0),\n",
    "            'leaveHomeTime': datetime.time(8, 30),\n",
    "            'comeHomeTime': datetime.time(18, 0),\n",
    "            'bedTime': datetime.time(22, 0)\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ktp-paper]",
   "language": "python",
   "name": "conda-env-ktp-paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
