{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haringey Study\n",
    "\n",
    "This notebook performs an energy agents study for Haringey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import datetime\n",
    "from itertools import chain, count, product\n",
    "import random\n",
    "import math\n",
    "import subprocess\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import pytz\n",
    "import requests_cache\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "%matplotlib inline\n",
    "\n",
    "from pytus2000 import read_diary_file, diary, read_individual_file, individual, read_diary_file_as_timeseries\n",
    "import pytus2000\n",
    "import people as ppl\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import ktp.census\n",
    "import ktp.synthpop\n",
    "import ktp.tus\n",
    "from ktp.synthpop import PeopleFeature, HouseholdFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "NAME ='haringey-scenario-lsoa-age'\n",
    "JAVA_HEAP_SIZE = 12 # GB\n",
    "NUMBER_PROCESSES = cpu_count() # max number of concurrent processes\n",
    "TIME_STEP_SIZE = datetime.timedelta(minutes=10)\n",
    "START_TIME = datetime.datetime(2005, 1, 3, 0, 0) # Monday\n",
    "NUMBER_TIME_STEPS = 6 * 24\n",
    "SET_POINT_WHILE_HOME = 22\n",
    "SET_POINT_WHILE_ASLEEP = 18\n",
    "METABOLIC_RATE_ACTIVE = 140\n",
    "METABOLIC_RATE_PASSIVE = 70\n",
    "METABOLIC_ADULT_RATE = 1.0\n",
    "METABOLIC_CHILD_RATE = 0.75\n",
    "SPATIAL_RESOLUTION = ktp.census.GeographicalLayer.WARD\n",
    "PEOPLE_FEATURES = [PeopleFeature.AGE]\n",
    "HOUSEHOLD_FEATURES = [HouseholdFeature.PSEUDO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FOLDER_PATH = Path('./build/')\n",
    "BUILD_FOLDER_PATH = Path('./build/') / NAME\n",
    "BUILD_FOLDER_PATH.mkdir(parents=True, exist_ok=True)\n",
    "TUS_DATA_FOLDER_PATH = Path('./data/UKDA-4504-tab/')\n",
    "MIDAS_DATABASE_PATH = Path('./data/Londhour.csv')\n",
    "\n",
    "PATH_TO_JAR = Path('../../energy-agents/target/energy-agents-1.0-SNAPSHOT-jar-with-dependencies.jar') # FIXME\n",
    "PATH_TO_INPUT_DB = (BUILD_FOLDER_PATH / '{}.db'.format(NAME)).absolute()\n",
    "PATH_TO_OUTPUT_DB = (BUILD_FOLDER_PATH / '{}-results.db'.format(NAME)).absolute()\n",
    "MARKOV_CHAIN_INDEX_TABLE_NAME = 'markovChains'\n",
    "DWELLINGS_TABLE_NAME = 'dwellings'\n",
    "PEOPLE_TABLE_NAME = 'people'\n",
    "ENVIRONMENT_TABLE_NAME = 'environment'\n",
    "PARAMETERS_TABLE_NAME = 'parameters'\n",
    "NUMBER_HOUSEHOLDS_HARINGEY = 101955\n",
    "NUMBER_USUAL_RESIDENTS_HARINGEY = 254926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PATH_TO_INPUT_DB.exists():\n",
    "    raise IOError('Input database already exists. Please delete or choose different name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed('haringey-scenario')\n",
    "pytus2000.set_cache_location(CACHE_FOLDER_PATH)\n",
    "requests_cache.install_cache((CACHE_FOLDER_PATH / 'web-cache').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_input_db(df, table_name):\n",
    "    disk_engine = sqlalchemy.create_engine('sqlite:///{}'.format(PATH_TO_INPUT_DB))\n",
    "    df.to_sql(name=table_name, con=disk_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress_bar(generator, progress_bar):\n",
    "    for elem in generator:\n",
    "        progress_bar.value += 1\n",
    "        yield elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(path_to_jar, path_to_input, path_to_output):\n",
    "    cmd = ['java', '-jar', '-Xmx{}g'.format(JAVA_HEAP_SIZE), str(path_to_jar), \n",
    "           '-i', str(path_to_input), '-o', str(path_to_output),\n",
    "           '-w', str(NUMBER_PROCESSES)]\n",
    "    popen = subprocess.Popen(\n",
    "        cmd, \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE,\n",
    "        universal_newlines=True,\n",
    "    )\n",
    "    for stdout_line in iter(popen.stdout.readline, \"\"):\n",
    "        print(stdout_line, end=\"\")\n",
    "    popen.stdout.close()\n",
    "    return_code = popen.wait()\n",
    "    if return_code:\n",
    "        raise subprocess.CalledProcessError(return_code, cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, clean, and map all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participants\n",
    "\n",
    "First off, let's define the group of people we are using from the UK Time Use Survey 2000 as seed for the synthetic population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_data = read_individual_file(TUS_DATA_FOLDER_PATH / 'tab' / 'individual_data_5.tab')\n",
    "## TODO filter city population\n",
    "## TODO remove invalid households\n",
    "age = individual_data.IAGE\n",
    "seed = pd.DataFrame(index=individual_data.index)\n",
    "for feature in PEOPLE_FEATURES:\n",
    "    seed[str(feature)] = feature.tus_value_to_ktp_value(individual_data[feature.tus_variable_name], age)\n",
    "for feature in HOUSEHOLD_FEATURES:\n",
    "    seed[str(feature)] = feature.tus_value_to_ktp_value(individual_data[feature.tus_variable_name])\n",
    "seed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} individuals in the seed have at least one missing feature '\n",
    "      'and will be removed.'.format(len(seed[seed.isnull().any(axis=1)].index)))\n",
    "seed.dropna(axis='index', how='any', inplace=True)\n",
    "assert not seed.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amend Seed by Metabolic Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.loc[individual_data.IAGE < 18, 'metabolic_rate_active'] = METABOLIC_RATE_ACTIVE * METABOLIC_CHILD_RATE\n",
    "seed.loc[individual_data.IAGE < 18, 'metabolic_rate_passive'] = METABOLIC_RATE_ACTIVE * METABOLIC_CHILD_RATE\n",
    "seed.loc[individual_data.IAGE > 18, 'metabolic_rate_active'] = METABOLIC_RATE_ACTIVE * METABOLIC_ADULT_RATE\n",
    "seed.loc[individual_data.IAGE > 18, 'metabolic_rate_passive'] = METABOLIC_RATE_PASSIVE * METABOLIC_ADULT_RATE\n",
    "# TODO add female metabolic rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Markov Chains\n",
    "\n",
    "Now that we have the participants, we can create markov chain for each type of citizen. A type is defined by all people features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_data = read_diary_file(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')\n",
    "diary_data_ts = read_diary_file_as_timeseries(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')[['activity', 'location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ts = pd.DataFrame({\n",
    "    'location': diary_data_ts.location.map(ktp.tus.LOCATION_MAP),\n",
    "    'activity': diary_data_ts.activity.map(ktp.tus.ACTIVITY_MAP)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Handle Unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ts.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simple_ts[(simple_ts.activity == ktp.tus.Activity.UNKNOWN) | (simple_ts.location == ktp.tus.Location.UNKNOWN)]) / len(simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5% of all entries are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts = simple_ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.replace(to_replace=[ktp.tus.Location.UNKNOWN, ktp.tus.Activity.UNKNOWN], value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknowns will be filled by forward fill. That is, whenever  an acticity/location is unknown it is expected that the last known activity/location is still valid. \n",
    "\n",
    "When doing that, it is important to not forward fill between diaries (all diaries are below each other). Hence, they must be grouped into diaries first and then forward filled. This will lead to the fact that not all Unknowns can be filled (the ones at the beginning of the day), but that is wanted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts = filled_simple_ts.groupby([filled_simple_ts.index.get_level_values(0), \n",
    "                                             filled_simple_ts.index.get_level_values(1), \n",
    "                                             filled_simple_ts.index.get_level_values(2), \n",
    "                                             filled_simple_ts.index.get_level_values(3)]).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO don't forward fill over too long durations, e.g. not more than 1-2h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining nans are filtered in the Filter section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to markov states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = ktp.tus.from_simplified_location_and_activity_to_people_model(filled_simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nan(markov_ts, diary_data):\n",
    "    \"\"\"Remove all diaries with at least one NaN.\"\"\"\n",
    "    nan_mask = markov_ts.groupby(by=lambda index: (index[0], index[1], index[2], index[3])).apply(lambda values: values.isnull().any())\n",
    "    return pd.DataFrame(markov_ts)[markov_ts.index.droplevel(4).isin(nan_mask[~nan_mask].index)]\n",
    "\n",
    "markov_ts = filter_nan(markov_ts, diary_data)\n",
    "assert not markov_ts.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster by synth pop categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_groups = seed.groupby([str(feature) for feature in PEOPLE_FEATURES])\n",
    "print(\"Dividing the seed into {} cluster.\".format(len(seed_groups.groups.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = diary_data[diary_data.DDAYW2 == diary.DDAYW2.WEEKDAY_MON___FRI]\n",
    "weekenddays = diary_data[diary_data.DDAYW2 == diary.DDAYW2.WEEKEND_DAY]\n",
    "\n",
    "progress_bar = ipywidgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(seed_groups.groups.keys()),\n",
    "    step=1,\n",
    "    description='Progress:',\n",
    "    bar_style='',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "display(progress_bar)\n",
    "\n",
    "with Pool(cpu_count()) as pool:\n",
    "    feature_combinations = seed_groups.groups.keys()\n",
    "    all_parameters = ( # imap_unordered allows only one parameter, hence the tuple\n",
    "        (markov_ts, \n",
    "         seed_groups.get_group(features),\n",
    "         features,\n",
    "         weekdays,\n",
    "         weekenddays,\n",
    "         TIME_STEP_SIZE)\n",
    "        for features in feature_combinations\n",
    "    )\n",
    "    markov_chains = dict(update_progress_bar(\n",
    "        pool.imap_unordered(ktp.tus.markov_chain_for_cluster, all_parameters),\n",
    "        progress_bar)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amend seed by markov chain attribute\n",
    "\n",
    "Now that we have calculated all markov chains, the id associated with the markov chain will be added as an attribute to the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_id(feature_values):\n",
    "    if not isinstance(feature_values, tuple): # single feature value\n",
    "        return feature_values.value\n",
    "    elif len(feature_values) == 2:\n",
    "        return pairing_function(feature_values[0].value, feature_values[1].value)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def pairing_function(x, y):\n",
    "    # cantor pairing function, http://stackoverflow.com/a/919661/1856079\n",
    "    return int(1/2 * (x + y) * (x + y + 1) + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed['markov_id'] = pd.Series(index=seed.index, data=0, dtype=np.int32)\n",
    "for feature_combination, index in seed_groups.groups.items():\n",
    "    seed.loc[index, 'markov_id'] = markov_id(feature_combination)\n",
    "    seed.loc[index, 'initial_activity'] = markov_chains[feature_combination].valid_states(START_TIME)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data_ppl = {feature: feature.read_census_data(SPATIAL_RESOLUTION) for feature in PEOPLE_FEATURES}\n",
    "for data in census_data_ppl.values():\n",
    "    assert data.sum().sum() == NUMBER_USUAL_RESIDENTS_HARINGEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data_hh = {feature: feature.read_census_data(SPATIAL_RESOLUTION) for feature in HOUSEHOLD_FEATURES}\n",
    "for data in census_data_hh.values():\n",
    "    assert data.sum().sum() == NUMBER_HOUSEHOLDS_HARINGEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare index\n",
    "sn1_plus_sn2 = seed.index.droplevel(2)\n",
    "seed = seed.copy()\n",
    "seed['household_id'] = list(sn1_plus_sn2)\n",
    "seed.reset_index(inplace=True)\n",
    "seed.rename(columns={'SN3': 'person_id'}, inplace=True)\n",
    "seed.set_index(['household_id', 'person_id'], inplace=True)\n",
    "seed.drop(['SN1', 'SN2'], axis=1, inplace=True)\n",
    "seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the iterative proportional fitting and create synthetic population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_hh_feature = list(census_data_hh.values())[0]\n",
    "regions = list(random_hh_feature.index)\n",
    "controls_hh = {region: {str(feature): census_data_hh[feature].ix[region, :] for feature in HOUSEHOLD_FEATURES}\n",
    "               for region in regions}\n",
    "controls_ppl = {region: {str(feature): census_data_ppl[feature].ix[region, :] for feature in PEOPLE_FEATURES}\n",
    "                for region in regions}\n",
    "number_households = {region: random_hh_feature.ix[region, :].sum() for region in regions}\n",
    "household_counter = count(start=1, step=1)\n",
    "household_ids = {region: [household_counter.__next__() for _ in range(number_households[region])]\n",
    "                for region in regions}\n",
    "random_numbers = {region: [random.uniform(0, 1) for _ in range(number_households[region])]\n",
    "                  for region in regions}\n",
    "hh_chunk_size = int(NUMBER_HOUSEHOLDS_HARINGEY / NUMBER_PROCESSES / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_progress_bar = ipywidgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(regions),\n",
    "    step=1,\n",
    "    description='Fitting:',\n",
    "    bar_style='',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "households_progress_bar = ipywidgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(regions),\n",
    "    step=1,\n",
    "    description='Househo:',\n",
    "    bar_style='',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "citizen_progress_bar = ipywidgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=math.ceil(NUMBER_HOUSEHOLDS_HARINGEY / hh_chunk_size),\n",
    "    step=1,\n",
    "    description='Citizen:',\n",
    "    bar_style='',\n",
    "    orientation='horizontal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_progress_bar.value = 0\n",
    "households_progress_bar.value = 0\n",
    "citizen_progress_bar.value = 0\n",
    "display(fitting_progress_bar)\n",
    "display(households_progress_bar)\n",
    "display(citizen_progress_bar)\n",
    "\n",
    "with Pool(NUMBER_PROCESSES) as pool:\n",
    "    hipf_params = ((seed, controls_hh[region], controls_ppl[region], region) for region in regions)\n",
    "    household_weights = dict(update_progress_bar(\n",
    "        pool.imap_unordered(ktp.synthpop.run_hipf, hipf_params), \n",
    "        fitting_progress_bar\n",
    "    ))\n",
    "    household_params = ((region, seed, household_weights[region], random_numbers[region], household_ids[region])\n",
    "                        for region in regions)\n",
    "    households = list(chain(*update_progress_bar(\n",
    "        pool.imap_unordered(ktp.synthpop.sample_households, household_params),\n",
    "        households_progress_bar\n",
    "    )))\n",
    "    household_chunks = [households[i:i + hh_chunk_size] for i in range(0, len(households), hh_chunk_size)]\n",
    "    citizens = list(chain(*update_progress_bar(\n",
    "        pool.imap_unordered(ktp.synthpop.sample_citizen, ((households, seed) for households in household_chunks)),\n",
    "        citizen_progress_bar\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(households) == NUMBER_HOUSEHOLDS_HARINGEY\n",
    "assert abs(len(citizens) - NUMBER_USUAL_RESIDENTS_HARINGEY) < 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Synthetic Population with Parameters from UKBuildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ; for the moment use random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformDistributedParameter():\n",
    "    \n",
    "    def __init__(self, expected_value, variation_in_percent):\n",
    "        self.__expected_value = expected_value\n",
    "        self.__random_max = expected_value * variation_in_percent / 100\n",
    "        \n",
    "    def sample(self):\n",
    "        return self.__expected_value + random.uniform(-self.__random_max, self.__random_max)\n",
    "   \n",
    "\n",
    "CONDITIONED_FLOOR_AREA = 100 # m^2\n",
    "HEAT_MASS_CAPACITY = UniformDistributedParameter(165000 * CONDITIONED_FLOOR_AREA, 0.0)\n",
    "HEAT_TRANSMISSION = UniformDistributedParameter(200, 0.0)\n",
    "MAX_HEATING_POWER = 10000\n",
    "INITIAL_TEMPERATURE = UniformDistributedParameter(22, 0.0)\n",
    "HEATING_CONTROL_STRATEGY = 'PRESENCE_TRIGGERED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwellings_df = pd.DataFrame(\n",
    "    index=[household.id for household in households],\n",
    "    data={\n",
    "        'heatMassCapacity': [HEAT_MASS_CAPACITY.sample() for unused in households],\n",
    "        'heatTransmission': [HEAT_TRANSMISSION.sample() for unused in households],\n",
    "        'maxHeatingPower': MAX_HEATING_POWER,\n",
    "        'initialTemperature': [INITIAL_TEMPERATURE.sample() for unused in households],\n",
    "        'conditionedFloorArea': CONDITIONED_FLOOR_AREA,\n",
    "        'heatingControlStrategy': HEATING_CONTROL_STRATEGY,\n",
    "        'region': [household.region for household in households]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citizen_df = pd.DataFrame(\n",
    "    index=list(range(len(citizens))),\n",
    "    data={\n",
    "        'markovChainId': [citizen.markovId for citizen in citizens],\n",
    "        'dwellingId': [citizen.householdId for citizen in citizens],\n",
    "        'initialActivity': [str(citizen.initialActivity) for citizen in citizens],\n",
    "        'activeMetabolicRate': [citizen.activeMetabolicRate for citizen in citizens],\n",
    "        'passiveMetabolicRate': [citizen.passiveMetabolicRate for citizen in citizens],\n",
    "        'randomSeed': [citizen.randomSeed for citizen in citizens]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_index = pd.Series(\n",
    "    {\n",
    "        markov_id(feature_combination): \"markov{}\".format(markov_id(feature_combination))\n",
    "        for feature_combination in markov_chains.keys()\n",
    "    }, \n",
    "    name='tablename'\n",
    ")\n",
    "df_to_input_db(markov_index, MARKOV_CHAIN_INDEX_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_combination, markov_chain in markov_chains.items():\n",
    "    df = markov_chain.to_dataframe()\n",
    "    df.fromActivity = [str(x) for x in df.fromActivity]\n",
    "    df.toActivity = [str(x) for x in df.toActivity]\n",
    "    df_to_input_db(df, markov_index[markov_id(feature_combination)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_input_db(dwellings_df, DWELLINGS_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_input_db(citizen_df, PEOPLE_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_parser(date, time):\n",
    "    month, day, year = [int(x) for x in date.split('/')]\n",
    "    hour, minute = [int(x) for x in time.split(':')]\n",
    "    return datetime.datetime(year, month, day, hour - 1, minute)\n",
    "\n",
    "temperature = pd.read_csv(\n",
    "    MIDAS_DATABASE_PATH, \n",
    "    skiprows=[0], \n",
    "    header=0, \n",
    "    parse_dates=[['Date (MM/DD/YYYY)', 'Time (HH:MM)']], \n",
    "    date_parser=date_parser,\n",
    "    index_col=[0]\n",
    ")\n",
    "temperature.rename(columns={'Dry-bulb (C)': 'temperature'}, inplace=True)\n",
    "temperature.index.name = 'index'\n",
    "df_to_input_db(temperature['temperature'].resample(TIME_STEP_SIZE).ffill(), ENVIRONMENT_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_input_db(\n",
    "    table_name=PARAMETERS_TABLE_NAME,\n",
    "    df=pd.DataFrame(\n",
    "        index=[1],\n",
    "        data={\n",
    "            'initialDatetime': START_TIME,\n",
    "            'timeStepSize_in_min': TIME_STEP_SIZE.total_seconds() / 60,\n",
    "            'numberTimeSteps': NUMBER_TIME_STEPS,\n",
    "            'setPointWhileHome': SET_POINT_WHILE_HOME,\n",
    "            'setPointWhileAsleep': SET_POINT_WHILE_ASLEEP,\n",
    "            'wakeUpTime': datetime.time(7, 0),\n",
    "            'leaveHomeTime': datetime.time(8, 30),\n",
    "            'comeHomeTime': datetime.time(18, 0),\n",
    "            'bedTime': datetime.time(22, 0)\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simulation(PATH_TO_JAR, PATH_TO_INPUT_DB, PATH_TO_OUTPUT_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_engine = sqlalchemy.create_engine('sqlite:///{}'.format(PATH_TO_OUTPUT_DB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('SELECT * FROM metadata', disk_engine, index_col='key', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermal_power = pd.read_sql_query('SELECT * FROM thermalPower', disk_engine, index_col='timestamp', parse_dates=True)\n",
    "thermal_power.index = pd.to_datetime(thermal_power.index * 1000 * 1000)\n",
    "thermal_power.index.name = 'datetime'\n",
    "thermal_power = thermal_power.pivot(columns='id')\n",
    "thermal_power.columns = thermal_power.columns.droplevel(0)\n",
    "thermal_power.name = 'thermal power'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwellings = pd.read_sql_query('SELECT * FROM dwellings', disk_engine, index_col='index')\n",
    "people = pd.read_sql_query('SELECT * FROM people', disk_engine, index_col='index')\n",
    "dwellings['householdSize'] = people.groupby('dwellingId').size()\n",
    "dwellings['average_power'] = thermal_power.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data = ktp.census.read_haringey_shape_file(SPATIAL_RESOLUTION)\n",
    "householdTypes = ktp.census.read_household_type_data(SPATIAL_RESOLUTION)\n",
    "age_structure = ktp.census.read_age_structure_data(SPATIAL_RESOLUTION)\n",
    "qualification_data = ktp.census.read_qualification_level_data(SPATIAL_RESOLUTION)\n",
    "economic_activity_data = ktp.census.read_economic_activity_data(SPATIAL_RESOLUTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGE_MAP = {\n",
    "    ktp.types.AgeStructure.AGE_0_TO_4: 2.5,\n",
    "    ktp.types.AgeStructure.AGE_5_TO_7: 6.5,\n",
    "    ktp.types.AgeStructure.AGE_8_TO_9: 9,\n",
    "    ktp.types.AgeStructure.AGE_10_TO_14: 12.5,\n",
    "    ktp.types.AgeStructure.AGE_15: 15.5,\n",
    "    ktp.types.AgeStructure.AGE_16_TO_17: 17,\n",
    "    ktp.types.AgeStructure.AGE_18_TO_19: 19,\n",
    "    ktp.types.AgeStructure.AGE_20_TO_24: 22.5,\n",
    "    ktp.types.AgeStructure.AGE_25_TO_29: 27.5,\n",
    "    ktp.types.AgeStructure.AGE_30_TO_44: 37.5,\n",
    "    ktp.types.AgeStructure.AGE_45_TO_59: 52.5,\n",
    "    ktp.types.AgeStructure.AGE_60_TO_64: 62.5,\n",
    "    ktp.types.AgeStructure.AGE_65_TO_74: 70,\n",
    "    ktp.types.AgeStructure.AGE_75_TO_84: 80,\n",
    "    ktp.types.AgeStructure.AGE_85_TO_89: 87.5,\n",
    "    ktp.types.AgeStructure.AGE_90_AND_OVER: 95 # FIXME\n",
    "}\n",
    "\n",
    "def meanAge(age_structure):\n",
    "    age_structure_num = age_structure.copy()\n",
    "    for col in age_structure:\n",
    "        age_structure_num[col] = age_structure[col] * AGE_MAP[col]\n",
    "    return age_structure_num.sum(axis=1) / age_structure.sum(axis=1)\n",
    "    \n",
    "    \n",
    "def percent_highest_qualification(qualification_data):\n",
    "    return qualification_data[ktp.types.Qualification.LEVEL_45] / qualification_data.sum(axis=1)\n",
    "\n",
    "\n",
    "def percent_economic_active(economic_activity_data):\n",
    "    total_active = economic_activity_data[[ktp.types.EconomicActivity.EMPLOYEE_PART_TIME, \n",
    "                                           ktp.types.EconomicActivity.EMPLOYEE_FULL_TIME,\n",
    "                                           ktp.types.EconomicActivity.SELF_EMPLOYED, \n",
    "                                           ktp.types.EconomicActivity.ACTIVE_FULL_TIME_STUDENT]].sum(axis=1)\n",
    "    return total_active / economic_activity_data.sum(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data['average_power'] = dwellings.groupby('region').average_power.mean()\n",
    "geo_data['number_households'] = householdTypes.sum(axis=1)\n",
    "geo_data['number citizens'] = age_structure.sum(axis=1)\n",
    "geo_data['avg household size'] = age_structure.sum(axis=1)/householdTypes.sum(axis=1)\n",
    "geo_data['avg age'] = meanAge(age_structure)\n",
    "geo_data['percent highest qual'] = percent_highest_qualification(qualification_data)\n",
    "geo_data['percent economic act'] = percent_economic_active(economic_activity_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = thermal_power\\\n",
    "    .groupby(axis=1, by=lambda id: dwellings.loc[id, 'region'])\\\n",
    "    .mean()\\\n",
    "    .plot(figsize=(14, 7), legend=None)\n",
    "_ = plt.ylabel('average thermal power per household [W]')\n",
    "_ = plt.title('Average of thermal power per household in different {}'.format(SPATIAL_RESOLUTION))\n",
    "fig = ax.get_figure()\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'thermal_power.png').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "sns.violinplot(data=dwellings.groupby('region').average_power.mean())\n",
    "_ = plt.ylabel('average thermal power per household [W]')\n",
    "_ = plt.xticks([])\n",
    "_ = plt.title(\"Distribution of average thermal power per household among {}\".format(SPATIAL_RESOLUTION))\n",
    "fig.savefig((BUILD_FOLDER_PATH / \"distributation-average-power.png\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandasplotting as gpdplt\n",
    "\n",
    "ax = gpdplt.plot_dataframe(\n",
    "    geo_data,\n",
    "    column='average_power',\n",
    "    categorical=False, \n",
    "    linewidth=0.2, \n",
    "    legend=True,\n",
    "    figsize=(14, 7),\n",
    "    cmap='viridis'\n",
    ")\n",
    "_ = plt.title(\"Average Thermal Power per Household in different {} [W]\".format(SPATIAL_RESOLUTION))\n",
    "_ = plt.xticks([])\n",
    "_ = plt.yticks([])\n",
    "fig = ax.get_figure()\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'thermal_power_choropleth.png').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power_region = dwellings.groupby('region').average_power.mean().max()\n",
    "min_power_region = dwellings.groupby('region').average_power.mean().min()\n",
    "print(max_power_region/min_power_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=dwellings, x='region', y='householdSize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(\n",
    "    data=geo_data, \n",
    "    vars=['average_power', 'avg household size', 'avg age', \n",
    "            'percent highest qual', 'percent economic act'])\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'pairwise-distributions.png').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ktp-paper]",
   "language": "python",
   "name": "conda-env-ktp-paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
