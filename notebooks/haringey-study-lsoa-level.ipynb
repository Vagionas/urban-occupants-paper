{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haringey Study\n",
    "\n",
    "This notebook performs an energy agents study for Haringey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import datetime\n",
    "from itertools import chain, count\n",
    "import random\n",
    "import math\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import requests_cache\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "from pytus2000 import read_diary_file, diary, read_individual_file, individual, read_diary_file_as_timeseries\n",
    "import pytus2000\n",
    "import people as ppl\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import ktp.census\n",
    "import ktp.synthpop\n",
    "import ktp.tus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BUILD_FOLDER_PATH = Path('./build/')\n",
    "TUS_DATA_FOLDER_PATH = Path('./data/UKDA-4504-tab/')\n",
    "MIDAS_DATABASE_PATH = Path('./data/Londhour.csv')\n",
    "\n",
    "PATH_TO_JAR = Path('../../energy-agents/target/energy-agents-1.0-SNAPSHOT-jar-with-dependencies.jar') # FIXME\n",
    "PATH_TO_INPUT_DB = (BUILD_FOLDER_PATH / ('haringey-scenario-lsoa.db')).absolute()\n",
    "PATH_TO_OUTPUT_DB = (BUILD_FOLDER_PATH / ('haringey-scenario-lsoa-results.db')).absolute()\n",
    "PATH_TO_INPUT_DB.parent.mkdir(parents=True, exist_ok=True)\n",
    "MARKOV_CHAIN_INDEX_TABLE_NAME = 'markovChains'\n",
    "DWELLINGS_TABLE_NAME = 'dwellings'\n",
    "PEOPLE_TABLE_NAME = 'people'\n",
    "ENVIRONMENT_TABLE_NAME = 'environment'\n",
    "PARAMETERS_TABLE_NAME = 'parameters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed('haringey-scenario-lsoa')\n",
    "pytus2000.set_cache_location(BUILD_FOLDER_PATH)\n",
    "requests_cache.install_cache((BUILD_FOLDER_PATH / 'web-cache').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_to_input_db(df, table_name):\n",
    "    disk_engine = sqlalchemy.create_engine('sqlite:///{}'.format(PATH_TO_INPUT_DB))\n",
    "    df.to_sql(name=table_name, con=disk_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_progress_bar(generator, progress_bar):\n",
    "    for elem in generator:\n",
    "        progress_bar.value += 1\n",
    "        yield elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_simulation(path_to_jar, path_to_input, path_to_output):\n",
    "    cmd = ['java', '-jar', path_to_jar, '-i', path_to_input, '-o', path_to_output]\n",
    "    popen = subprocess.Popen(\n",
    "        cmd, \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE,\n",
    "        universal_newlines=True,\n",
    "        \n",
    "    )\n",
    "    for stdout_line in iter(popen.stdout.readline, \"\"):\n",
    "        print(stdout_line, end=\"\")\n",
    "    popen.stdout.close()\n",
    "    return_code = popen.wait()\n",
    "    if return_code:\n",
    "        raise subprocess.CalledProcessError(return_code, cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, clean, and map all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUMBER_HOUSEHOLDS_HARINGEY = 101955\n",
    "NUMBER_USUAL_RESIDENTS_HARINGEY = 254926\n",
    "TIME_STEP_SIZE = datetime.timedelta(minutes=10)\n",
    "START_TIME = datetime.datetime(2005, 1, 1, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participants\n",
    "\n",
    "First off, let's define the group of people we are using from the UK Time Use Survey 2000 as seed for the synthetic population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "individual_data = read_individual_file(TUS_DATA_FOLDER_PATH / 'tab' / 'individual_data_5.tab')\n",
    "## TODO filter city population\n",
    "seed = pd.DataFrame(index=individual_data.index, columns=['labour', 'qualification', 'age', 'hhtype'])\n",
    "seed.labour = individual_data.ECONACT2.map(ktp.tus.LABOUR_MAP)\n",
    "seed.labour[individual_data.IAGE > 74] = ktp.census.Labour.ABOVE_74\n",
    "seed.qualification = individual_data.HIQUAL4.map(ktp.tus.QUALIFICATION_MAP)\n",
    "seed['age'] = individual_data.IAGE.copy()\n",
    "seed['hhtype'] = individual_data.HHTYPE4.map(ktp.tus.HOUSEHOLDTYPE_MAP)\n",
    "seed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed.dropna(axis='index', how='any', inplace=True)\n",
    "assert not seed.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A household is invalid if the amount of individuals we have do not match the household type. For example, a couple without children household must have exactly two individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "household_types = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.first()\n",
    "household_sizes = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.count()\n",
    "mask_couples_children = household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN) & (household_sizes <= 2)]\n",
    "mask_couples_no_children = household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN) & (household_sizes != 2)]\n",
    "invalids = (\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN) & (household_sizes <= 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN) & (household_sizes != 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.LONE_PARENT_WITH_DEPENDENT_CHILDREN) & (household_sizes < 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.MULTI_PERSON_HOUSEHOLD) & (household_sizes <= 2)]\n",
    ")\n",
    "\n",
    "seed.drop(labels=invalids.index, level=None, inplace=True)\n",
    "\n",
    "\n",
    "print(\"{} households are invalid and were removed.\".format(invalids.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test all input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert not ((seed.labour == ktp.census.Labour.ABOVE_74) & (seed.age <= 74)).any()\n",
    "assert not ((seed.labour == ktp.census.Labour.BELOW_16) & (seed.age >= 16)).any()\n",
    "assert not ((seed.labour == ktp.census.Labour.BELOW_16) & (seed.qualification != ktp.census.Qualification.BELOW_16)).any()\n",
    "assert not ((seed.labour != ktp.census.Labour.BELOW_16) & (seed.qualification == ktp.census.Qualification.BELOW_16)).any()\n",
    "\n",
    "household_types = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.first()\n",
    "household_sizes = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.count()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.ONE_PERSON_HOUSEHOLD] == 1).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN] > 2).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.LONE_PARENT_WITH_DEPENDENT_CHILDREN] > 1).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN] == 2).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.MULTI_PERSON_HOUSEHOLD] > 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Markov Chains\n",
    "\n",
    "Now that we have the participants, we can create markov chain for each type of citizen. A type is defined by two attributes:\n",
    "\n",
    "* the current work status; 'labour' in the following\n",
    "* the highest qualification received, 'qualification' in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diary_data = read_diary_file(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')\n",
    "diary_data_ts = read_diary_file_as_timeseries(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')[['activity', 'location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simple_ts = pd.DataFrame({\n",
    "    'location': diary_data_ts.location.map(ktp.tus.LOCATION_MAP),\n",
    "    'activity': diary_data_ts.activity.map(ktp.tus.ACTIVITY_MAP)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Handle Unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_ts.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(simple_ts[(simple_ts.activity == ktp.tus.Activity.UNKNOWN) | (simple_ts.location == ktp.tus.Location.UNKNOWN)]) / len(simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5% of all entries are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filled_simple_ts = simple_ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filled_simple_ts.replace(to_replace=[ktp.tus.Location.UNKNOWN, ktp.tus.Activity.UNKNOWN], value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknowns will be filled by forward fill. That is, whenever  an acticity/location is unknown it is expected that the last known activity/location is still valid. \n",
    "\n",
    "When doing that, it is important to not forward fill between diaries (all diaries are below each other). Hence, they must be grouped into diaries first and then forward filled. This will lead to the fact that not all Unknowns can be filled (the ones at the beginning of the day), but that is wanted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filled_simple_ts = filled_simple_ts.groupby([filled_simple_ts.index.get_level_values(0), \n",
    "                                             filled_simple_ts.index.get_level_values(1), \n",
    "                                             filled_simple_ts.index.get_level_values(2), \n",
    "                                             filled_simple_ts.index.get_level_values(3)]).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO don't forward fill over too long durations, e.g. not more than 1-2h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining nans are filtered in the Filter section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to markov states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markov_ts = ktp.tus.from_simplified_location_and_activity_to_people_model(filled_simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_nan(markov_ts, diary_data):\n",
    "    \"\"\"Remove all diaries with at least one NaN.\"\"\"\n",
    "    nan_mask = markov_ts.groupby(by=lambda index: (index[0], index[1], index[2], index[3])).apply(lambda values: values.isnull().any())\n",
    "    return pd.DataFrame(markov_ts)[markov_ts.index.droplevel(4).isin(nan_mask[~nan_mask].index)]\n",
    "\n",
    "markov_ts = filter_nan(markov_ts, diary_data)\n",
    "assert not markov_ts.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster by synth pop categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed_groups = seed.groupby(['labour', 'qualification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def markov_chain_for_group_of_people(markov_ts, group_of_people, weekdays, weekenddays):\n",
    "    # filter by people\n",
    "    people_mask = markov_ts.index.droplevel([3, 4]).isin(group_of_people.index)\n",
    "    filtered_markov = pd.DataFrame(markov_ts)[people_mask]\n",
    "    # filter by weekday\n",
    "    weekday_mask = filtered_markov.index.droplevel([4]).isin(weekdays.index)\n",
    "    filtered_markov_weekday = filtered_markov[weekday_mask]\n",
    "    # filter by weekend\n",
    "    weekend_mask = filtered_markov.index.droplevel([4]).isin(weekenddays.index)\n",
    "    filtered_markov_weekend = filtered_markov[weekend_mask]\n",
    "    return ppl.WeekMarkovChain(\n",
    "        weekday_time_series=filtered_markov_weekday.unstack(level=[0, 1, 2, 3]),\n",
    "        weekend_time_series=filtered_markov_weekend.unstack(level=[0, 1, 2, 3]),\n",
    "        time_step_size=TIME_STEP_SIZE\n",
    "    )\n",
    "\n",
    "\n",
    "def people_group(labour, qualification):\n",
    "    group = seed_groups.get_group((labour, qualification))\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weekdays = diary_data[diary_data.DDAYW2 == diary.DDAYW2.WEEKDAY_MON___FRI]\n",
    "weekenddays = diary_data[diary_data.DDAYW2 == diary.DDAYW2.WEEKEND_DAY]\n",
    "\n",
    "progress_bar = ipywidgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len([qual for qual in ktp.census.Qualification]) * len([lab for lab in ktp.census.Labour]),\n",
    "    step=1,\n",
    "    description='Progress:',\n",
    "    bar_style='',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "display(progress_bar)\n",
    "\n",
    "markov_chains = {\n",
    "    (labour, qualification): markov_chain_for_group_of_people(\n",
    "        markov_ts=markov_ts, \n",
    "        group_of_people=people_group(labour, qualification),\n",
    "        weekdays=weekdays,\n",
    "        weekenddays=weekenddays\n",
    "    ) if (labour, qualification) in seed_groups.groups.keys() else None\n",
    "    for labour in ktp.census.Labour\n",
    "    for qualification in update_progress_bar(ktp.census.Qualification, progress_bar)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amend seed by markov chain attribute\n",
    "\n",
    "Now that we have calculated all markov chains, the id associated with the markov chain will be added as an attribute to the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def markov_id(labour, qualification):\n",
    "    # cantor pairing function, http://stackoverflow.com/a/919661/1856079\n",
    "    x = labour.value\n",
    "    y = qualification.value\n",
    "    return int(1/2 * (x + y) * (x + y + 1) + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed['markov_id'] = seed.apply(\n",
    "    lambda row: markov_id(row.labour, row.qualification), \n",
    "    axis=1\n",
    ")\n",
    "seed['initial_activity'] = seed.apply(\n",
    "    lambda row: markov_chains[(row.labour, row.qualification)].valid_states(START_TIME)[0], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO data should be retrieved using API\n",
    "PATH_TO_USUAL_RESIDENTS = Path('./data/census/usual_residents.csv')\n",
    "PATH_TO_ECONOMIC_ACTIVITY = Path('./data/census/economic_activity.csv')\n",
    "PATH_TO_HOUSEHOLD_TYPE = Path('./data/census/household_type.csv')\n",
    "PATH_TO_QUALIFICATION = Path('./data/census/qualification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usual_residents = pd.read_csv(PATH_TO_USUAL_RESIDENTS, skiprows=8, skipfooter=5, engine='python', index_col='mnemonic')\n",
    "usual_residents.drop('2011 super output area - lower layer', axis=1, inplace=True)\n",
    "assert usual_residents.sum().sum() == NUMBER_USUAL_RESIDENTS_HARINGEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "household_data = pd.read_csv(PATH_TO_HOUSEHOLD_TYPE, skiprows=8, skipfooter=5, engine='python', index_col='mnemonic')\n",
    "household_data.drop('2011 super output area - lower layer', axis=1, inplace=True)\n",
    "household_data = household_data.rename(columns=ktp.census.HOUSEHOLDTYPE_MAP).groupby(lambda x:x, axis=1).sum()\n",
    "assert household_data.sum().sum() == NUMBER_HOUSEHOLDS_HARINGEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qualification_map = {\n",
    "    'No qualifications': ktp.census.Qualification.NO_QUALIFICATIONS,\n",
    "    'Highest level of qualification: Level 1 qualifications': ktp.census.Qualification.LEVEL_1,\n",
    "    'Highest level of qualification: Level 2 qualifications': ktp.census.Qualification.LEVEL_2,\n",
    "    'Highest level of qualification: Apprenticeship': ktp.census.Qualification.APPRENTICESHIP,\n",
    "    'Highest level of qualification: Level 3 qualifications': ktp.census.Qualification.LEVEL_3,\n",
    "    'Highest level of qualification: Level 4 qualifications and above': ktp.census.Qualification.LEVEL_45,\n",
    "    'Highest level of qualification: Other qualifications': ktp.census.Qualification.OTHER_QUALIFICATION\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qualification_data = pd.read_csv(PATH_TO_QUALIFICATION, skiprows=8, skipfooter=5, engine='python', index_col='mnemonic')\n",
    "qualification_data.drop('2011 super output area - lower layer', axis=1, inplace=True)\n",
    "qualification_data = qualification_data.rename(columns=qualification_map).groupby(lambda x:x, axis=1).sum()\n",
    "assert qualification_data.sum().sum() == usual_residents.ix[:, 'Age 16 to 17':].sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualification data is available for every usual resident starting from age 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "younger_than_sixteen = usual_residents.ix[:, :'Age 15'].sum(axis=1)\n",
    "qualification_data[ktp.census.Qualification.BELOW_16] = younger_than_sixteen\n",
    "assert qualification_data.sum().sum() == usual_residents.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labour_data = pd.read_csv(PATH_TO_ECONOMIC_ACTIVITY, skiprows=8, skipfooter=5, engine='python', index_col='mnemonic')\n",
    "labour_data.drop('2011 super output area - lower layer', axis=1, inplace=True)\n",
    "labour_data = labour_data.rename(columns=ktp.census.LABOUR_MAP).groupby(lambda x:x, axis=1).sum()\n",
    "assert labour_data.sum().sum() == usual_residents.ix[:, 'Age 16 to 17':'Age 65 to 74'].sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labour data is available for every usual resident between age 16 and 74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labour_data[ktp.census.Labour.BELOW_16] = younger_than_sixteen\n",
    "labour_data[ktp.census.Labour.ABOVE_74] = usual_residents.ix[:, 'Age 75 to 84':].sum(axis=1)\n",
    "assert labour_data.sum().sum() == usual_residents.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare index\n",
    "sn1_plus_sn2 = seed.index.droplevel(2)\n",
    "seed = seed.copy()\n",
    "seed['household_id'] = list(sn1_plus_sn2)\n",
    "seed.reset_index(inplace=True)\n",
    "seed.rename(columns={'SN3': 'person_id'}, inplace=True)\n",
    "seed.set_index(['household_id', 'person_id'], inplace=True)\n",
    "seed.drop(['SN1', 'SN2'], axis=1, inplace=True)\n",
    "seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the iterative proportional fitting and create synthetic population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Dwelling = namedtuple('Dwelling', ['id', 'seedId','householdType', 'region'])\n",
    "DWELLING_COUNTER = count(0)\n",
    "Citizen = namedtuple('Citizen', ['dwellingId', 'markovId', 'initialActivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def synthetic_dwelling_generator(household_weights, seed, number_households, region):\n",
    "    norm_household_weights = household_weights.copy()\n",
    "    norm_household_weights = household_weights / household_weights.sum()\n",
    "    cum_norm_household_weights = norm_household_weights.cumsum()\n",
    "    assert math.isclose(norm_household_weights.sum(), 1, abs_tol=0.001) \n",
    "    for i in range(1, number_households + 1):\n",
    "        household_id = sample_household(cum_norm_household_weights)\n",
    "        yield Dwelling(DWELLING_COUNTER.__next__(), household_id, seed.ix[(household_id), :].iloc[0].hhtype, region)\n",
    "\n",
    "        \n",
    "def sample_household(cumulated_household_weights):\n",
    "    random_number = random.uniform(0, 1)\n",
    "    return cumulated_household_weights[cumulated_household_weights >= random_number].index[0]\n",
    "\n",
    "\n",
    "def synthetic_population_generator(dwellings, seed):\n",
    "    for dwelling in dwellings:\n",
    "        inhabitants = seed.ix[dwelling.seedId, :]\n",
    "        for index, row in inhabitants.iterrows():\n",
    "            yield Citizen(\n",
    "                dwellingId=dwelling.id, \n",
    "                markovId=row.markov_id,\n",
    "                initialActivity=row.initial_activity\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hipf(region):\n",
    "    number_households = household_data.ix[region, :].sum()\n",
    "    household_weights = ktp.synthpop.fit_hipf(\n",
    "        reference_sample=seed,\n",
    "        controls_households={'hhtype': household_data.ix[region, :].to_dict()},\n",
    "        controls_individuals={\n",
    "            'labour': labour_data.ix[region, :].to_dict(),\n",
    "            'qualification': qualification_data.ix[region, :].to_dict()\n",
    "        },\n",
    "        residuals_tol=0.0001,\n",
    "        weights_tol=0.0001,\n",
    "        maxiter=100\n",
    "    )\n",
    "    assert number_households - household_weights.sum() < 0.1\n",
    "    assert not any(household_weights.isnull())\n",
    "    return synthetic_dwelling_generator(\n",
    "        household_weights=household_weights, \n",
    "        seed=seed, \n",
    "        number_households=number_households,\n",
    "        region=region\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "progress_bar = ipywidgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(household_data.index),\n",
    "    step=1,\n",
    "    description='Progress:',\n",
    "    bar_style='',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "display(progress_bar)\n",
    "\n",
    "dwellings = list(chain(*(hipf(region) for region in update_progress_bar(household_data.index, progress_bar))))\n",
    "citizens = list(synthetic_population_generator(dwellings, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(dwellings) == NUMBER_HOUSEHOLDS_HARINGEY\n",
    "assert abs(len(citizens) - NUMBER_USUAL_RESIDENTS_HARINGEY) < 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Synthetic Population with Parameters from UKBuildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO ; for the moment use random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UniformDistributedParameter():\n",
    "    \n",
    "    def __init__(self, expected_value, variation_in_percent):\n",
    "        self.__expected_value = expected_value\n",
    "        self.__random_max = expected_value * variation_in_percent / 100\n",
    "        \n",
    "    def sample(self):\n",
    "        return self.__expected_value + random.uniform(-self.__random_max, self.__random_max)\n",
    "   \n",
    "\n",
    "CONDITIONED_FLOOR_AREA = 100 # m^2\n",
    "HEAT_MASS_CAPACITY = UniformDistributedParameter(165000 * CONDITIONED_FLOOR_AREA, 0.0)\n",
    "HEAT_TRANSMISSION = UniformDistributedParameter(200, 0.0)\n",
    "MAX_HEATING_POWER = 10000\n",
    "MAX_COOLING_POWER = -10000\n",
    "INITIAL_TEMPERATURE = UniformDistributedParameter(22, 0.0)\n",
    "HEATING_CONTROL_STRATEGY = 'PRESENCE_TRIGGERED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dwellings_df = pd.DataFrame(\n",
    "    index=[dwelling.id for dwelling in dwellings],\n",
    "    data={\n",
    "        'heatMassCapacity': [HEAT_MASS_CAPACITY.sample() for unused in dwellings],\n",
    "        'heatTransmission': [HEAT_TRANSMISSION.sample() for unused in dwellings],\n",
    "        'maxHeatingPower': MAX_HEATING_POWER,\n",
    "        'maxCoolingPower': MAX_COOLING_POWER,\n",
    "        'initialTemperature': [INITIAL_TEMPERATURE.sample() for unused in dwellings],\n",
    "        'conditionedFloorArea': CONDITIONED_FLOOR_AREA,\n",
    "        'heatingControlStrategy': HEATING_CONTROL_STRATEGY,\n",
    "        'region': [dwelling.region for dwelling in dwellings]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citizen_df = pd.DataFrame(\n",
    "    index=list(range(len(citizens))),\n",
    "    data={\n",
    "        'markovChainId': [citizen.markovId for citizen in citizens],\n",
    "        'dwellingId': [citizen.dwellingId for citizen in citizens],\n",
    "        'initialActivity': [str(citizen.initialActivity) for citizen in citizens]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markov_index = pd.Series(\n",
    "    {\n",
    "        markov_id(labour, qualification): \"markov{}\".format(markov_id(labour, qualification))\n",
    "        for labour, qualification in markov_chains.keys() if markov_chains[(labour, qualification)] is not None\n",
    "    }, \n",
    "    name='tablename'\n",
    ")\n",
    "df_to_input_db(markov_index, MARKOV_CHAIN_INDEX_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, markov_chain in markov_chains.items():\n",
    "    if markov_chain is None:\n",
    "        continue\n",
    "    labour, qualification = key\n",
    "    df = markov_chain.to_dataframe()\n",
    "    df.fromActivity = [str(x) for x in df.fromActivity]\n",
    "    df.toActivity = [str(x) for x in df.toActivity]\n",
    "    df_to_input_db(df, markov_index[markov_id(labour, qualification)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_to_input_db(dwellings_df, DWELLINGS_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_to_input_db(citizen_df, PEOPLE_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def date_parser(date, time):\n",
    "    month, day, year = [int(x) for x in date.split('/')]\n",
    "    hour, minute = [int(x) for x in time.split(':')]\n",
    "    return datetime.datetime(year, month, day, hour - 1, minute)\n",
    "\n",
    "temperature = pd.read_csv(\n",
    "    MIDAS_DATABASE_PATH, \n",
    "    skiprows=[0], \n",
    "    header=0, \n",
    "    parse_dates=[['Date (MM/DD/YYYY)', 'Time (HH:MM)']], \n",
    "    date_parser=date_parser,\n",
    "    index_col=[0]\n",
    ")\n",
    "temperature.rename(columns={'Dry-bulb (C)': 'temperature'}, inplace=True)\n",
    "temperature.index.name = 'index'\n",
    "df_to_input_db(temperature['temperature'].resample(TIME_STEP_SIZE).ffill(), ENVIRONMENT_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_to_input_db(\n",
    "    table_name=PARAMETERS_TABLE_NAME,\n",
    "    df=pd.DataFrame(\n",
    "        index=[1],\n",
    "        data={\n",
    "            'initialDatetime': START_TIME,\n",
    "            'timeStepSize_in_min': TIME_STEP_SIZE.total_seconds() / 60,\n",
    "            'numberTimeSteps': 6 * 24,\n",
    "            'randomSeed': 123456789,\n",
    "            'setPointWhileHome': 22.0,\n",
    "            'setPointWhileAsleep': 18.0,\n",
    "            'wakeUpTime': datetime.time(7, 0),\n",
    "            'leaveHomeTime': datetime.time(8, 30),\n",
    "            'comeHomeTime': datetime.time(18, 0),\n",
    "            'bedTime': datetime.time(22, 0)\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_simulation(PATH_TO_JAR, PATH_TO_INPUT_DB, PATH_TO_OUTPUT_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ktp-paper]",
   "language": "python",
   "name": "conda-env-ktp-paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
