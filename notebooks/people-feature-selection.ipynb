{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import requests_cache\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "%matplotlib inline\n",
    "\n",
    "from pytus2000 import read_diary_file, diary, read_individual_file, individual, read_diary_file_as_timeseries\n",
    "import pytus2000\n",
    "import people as ppl\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import ktp.tus\n",
    "import ktp.census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_FOLDER_PATH = Path('./build/')\n",
    "TUS_DATA_FOLDER_PATH = Path('./data/UKDA-4504-tab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytus2000.set_cache_location(BUILD_FOLDER_PATH)\n",
    "requests_cache.install_cache((BUILD_FOLDER_PATH / 'web-cache').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_data = read_individual_file(TUS_DATA_FOLDER_PATH / 'tab' / 'individual_data_5.tab')\n",
    "## TODO filter city population\n",
    "seed = pd.DataFrame(index=individual_data.index, columns=['labour', 'qualification', 'age', 'hhtype'])\n",
    "seed.labour = individual_data.ECONACT2.map(ktp.tus.ECONOMIC_ACTIVITY_MAP)\n",
    "seed.labour[individual_data.IAGE > 74] = ktp.census.EconomicActivity.ABOVE_74\n",
    "seed.qualification = individual_data.HIQUAL4.map(ktp.tus.QUALIFICATION_MAP)\n",
    "seed['age'] = individual_data.IAGE.copy()\n",
    "seed['hhtype'] = individual_data.HHTYPE4.map(ktp.tus.HOUSEHOLDTYPE_MAP)\n",
    "## TODO add more feature, all that are in census data as well\n",
    "seed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.dropna(axis='index', how='any', inplace=True)\n",
    "assert not seed.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_types = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.first()\n",
    "household_sizes = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.count()\n",
    "mask_couples_children = household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN) & (household_sizes <= 2)]\n",
    "mask_couples_no_children = household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN) & (household_sizes != 2)]\n",
    "invalids = (\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN) & (household_sizes <= 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN) & (household_sizes != 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.LONE_PARENT_WITH_DEPENDENT_CHILDREN) & (household_sizes < 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.MULTI_PERSON_HOUSEHOLD) & (household_sizes <= 2)]\n",
    ")\n",
    "\n",
    "seed.drop(labels=invalids.index, level=None, inplace=True)\n",
    "\n",
    "\n",
    "print(\"{} households are invalid and were removed.\".format(invalids.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not ((seed.labour == ktp.census.EconomicActivity.ABOVE_74) & (seed.age <= 74)).any()\n",
    "assert not ((seed.labour == ktp.census.EconomicActivity.BELOW_16) & (seed.age >= 16)).any()\n",
    "assert not ((seed.labour == ktp.census.EconomicActivity.BELOW_16) & (seed.qualification != ktp.census.Qualification.BELOW_16)).any()\n",
    "assert not ((seed.labour != ktp.census.EconomicActivity.BELOW_16) & (seed.qualification == ktp.census.Qualification.BELOW_16)).any()\n",
    "\n",
    "household_types = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.first()\n",
    "household_sizes = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.count()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.ONE_PERSON_HOUSEHOLD] == 1).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN] > 2).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.LONE_PARENT_WITH_DEPENDENT_CHILDREN] > 1).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN] == 2).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.MULTI_PERSON_HOUSEHOLD] > 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_data = read_diary_file(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')\n",
    "diary_data_ts = read_diary_file_as_timeseries(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')[['activity', 'location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ts = pd.DataFrame({\n",
    "    'location': diary_data_ts.location.map(ktp.tus.LOCATION_MAP),\n",
    "    'activity': diary_data_ts.activity.map(ktp.tus.ACTIVITY_MAP)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ts.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simple_ts[(simple_ts.activity == ktp.tus.Activity.UNKNOWN) | (simple_ts.location == ktp.tus.Location.UNKNOWN)]) / len(simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5% of all entries are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts = simple_ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.replace(to_replace=[ktp.tus.Location.UNKNOWN, ktp.tus.Activity.UNKNOWN], value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknowns will be filled by forward fill. That is, whenever  an acticity/location is unknown it is expected that the last known activity/location is still valid. \n",
    "\n",
    "When doing that, it is important to not forward fill between diaries (all diaries are below each other). Hence, they must be grouped into diaries first and then forward filled. This will lead to the fact that not all Unknowns can be filled (the ones at the beginning of the day), but that is wanted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts = filled_simple_ts.groupby([filled_simple_ts.index.get_level_values(0), \n",
    "                                             filled_simple_ts.index.get_level_values(1), \n",
    "                                             filled_simple_ts.index.get_level_values(2), \n",
    "                                             filled_simple_ts.index.get_level_values(3)]).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO don't forward fill over too long durations, e.g. not more than 1-2h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining nans are filtered in the Filter section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map to markov states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = ktp.tus.from_simplified_location_and_activity_to_people_model(filled_simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nan(markov_ts, diary_data):\n",
    "    \"\"\"Remove all diaries with at least one NaN.\"\"\"\n",
    "    nan_mask = markov_ts.groupby(by=lambda index: (index[0], index[1], index[2], index[3])).apply(lambda values: values.isnull().any())\n",
    "    return pd.DataFrame(markov_ts)[markov_ts.index.droplevel(4).isin(nan_mask[~nan_mask].index)]\n",
    "\n",
    "def remove_people_not_in_seed(markov_ts, seed):\n",
    "    mask = markov_ts.index.droplevel([3, 4]).isin(seed.index)\n",
    "    return markov_ts[mask]\n",
    "\n",
    "\n",
    "def filter_all_people_for_which_less_than_two_diaries_exist(markov_ts):\n",
    "    valid_mask = markov_ts.groupby([markov_ts.index.get_level_values(0), \n",
    "                                    markov_ts.index.get_level_values(1), \n",
    "                                    markov_ts.index.get_level_values(2)]).apply(lambda values: len(values) == 24 * 6 * 2)\n",
    "    return markov_ts[markov_ts.index.droplevel([3, 4]).isin(valid_mask[valid_mask].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = filter_nan(markov_ts, diary_data)\n",
    "assert not markov_ts.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = remove_people_not_in_seed(markov_ts, seed)\n",
    "markov_ts = filter_all_people_for_which_less_than_two_diaries_exist(markov_ts)\n",
    "seed = seed[seed.index.isin(markov_ts.index.droplevel([3, 4]))]\n",
    "assert len(seed.index) * 2 * 24 * 6 == len(markov_ts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO exchange SN4 with day type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add DayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = diary_data[diary_data.DDAYW2 == diary.DDAYW2.WEEKDAY_MON___FRI]\n",
    "markov_ts['daytype'] = 'weekend'\n",
    "markov_ts.loc[markov_ts.index.droplevel(4).isin(weekdays.index), 'daytype'] = 'weekday'\n",
    "markov_ts = markov_ts.reset_index(level=[3, 4]).set_index(['daytype', 'time_of_day'], append=True)\n",
    "markov_ts.drop('SN4', axis=1, inplace=True)\n",
    "markov_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = markov_ts.unstack([0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from http://stackoverflow.com/a/39266194/1856079\n",
    "import scipy.stats as ss\n",
    "\n",
    "def cramers_corrected_stat(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramers_corrected_stat(pd.crosstab(seed['labour'], seed['qualification']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.get_dummies(seed['hhtype'], columns=['hhtype']).corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(pd.get_dummies(seed, columns=['labour', 'hhtype', 'qualification']).corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts.ix['weekend', 10].map(lambda x: x.value).plot()\n",
    "_ = plt.yticks([1, 2, 3, 4, 5], [x for x in ppl.Activity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_corr(index, markov_ts, feature):\n",
    "    time_slice = markov_ts.iloc[index, :]\n",
    "    time_slice.index = feature.index\n",
    "    return cramers_corrected_stat(pd.crosstab(feature, time_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_function(x, y):\n",
    "    # cantor pairing function, http://stackoverflow.com/a/919661/1856079\n",
    "    return int(1/2 * (x + y) * (x + y + 1) + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_corr = pd.DataFrame({\n",
    "    'qualification': [timestep_corr(i, markov_ts, seed.qualification) for i in range(144*2)],\n",
    "    'labour': [timestep_corr(i, markov_ts, seed.labour) for i in range(144*2)],\n",
    "    'hhtype': [timestep_corr(i, markov_ts, seed.hhtype) for i in range(144*2)],\n",
    "    'q+l': [timestep_corr(i, markov_ts, seed.apply(lambda row: pairing_function(row.qualification.value, row.labour.value), axis=1)) \n",
    "            for i in range(144*2)],\n",
    "    'q+h': [timestep_corr(i, markov_ts, seed.apply(lambda row: pairing_function(row.qualification.value, row.hhtype.value), axis=1)) \n",
    "            for i in range(144*2)],\n",
    "    'l+h': [timestep_corr(i, markov_ts, seed.apply(lambda row: pairing_function(row.labour.value, row.hhtype.value), axis=1)) \n",
    "            for i in range(144*2)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ts_corr.plot(figsize=(14, 7))\n",
    "_ = plt.title(\"Cramer's phi for each time step of people model\")\n",
    "_ = plt.ylabel(\"Cramer's phi\")\n",
    "_ = plt.xlabel(\"time step\")\n",
    "fig = ax.get_figure()\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'markov_ts_cramer.png').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ktp-paper]",
   "language": "python",
   "name": "conda-env-ktp-paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
