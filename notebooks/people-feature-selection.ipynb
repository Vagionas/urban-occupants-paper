{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import requests_cache\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "%matplotlib inline\n",
    "\n",
    "from pytus2000 import read_diary_file, diary, read_individual_file, individual, read_diary_file_as_timeseries\n",
    "import pytus2000\n",
    "import people as ppl\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import ktp.tus\n",
    "import ktp.census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_FOLDER_PATH = Path('./build/')\n",
    "TUS_DATA_FOLDER_PATH = Path('./data/UKDA-4504-tab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytus2000.set_cache_location(BUILD_FOLDER_PATH)\n",
    "requests_cache.install_cache((BUILD_FOLDER_PATH / 'web-cache').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['ageGroup', 'personalIncome', 'care',\n",
    "                'qualification', 'economicActivity', 'hhtype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_data = read_individual_file(TUS_DATA_FOLDER_PATH / 'tab' / 'individual_data_5.tab')\n",
    "## TODO filter city population\n",
    "seed = pd.DataFrame(index=individual_data.index, columns=all_features)\n",
    "seed.economicActivity = individual_data.ECONACT2.map(ktp.tus.ECONOMIC_ACTIVITY_MAP)\n",
    "seed.economicActivity[individual_data.IAGE > 74] = ktp.census.EconomicActivity.ABOVE_74\n",
    "seed.qualification = individual_data.HIQUAL4.map(ktp.tus.QUALIFICATION_MAP)\n",
    "seed['age'] = individual_data.IAGE.copy()\n",
    "seed['hhtype'] = individual_data.HHTYPE4.map(ktp.tus.HOUSEHOLDTYPE_MAP)\n",
    "seed['ageGroup'] = individual_data.IAGEGRP.copy()\n",
    "seed['personalIncome'] = individual_data.TOTPINC.copy()\n",
    "seed['SIC'] = individual_data.SIC.copy()\n",
    "seed['SIC2'] = individual_data.SIC2.copy()\n",
    "seed['SOC'] = individual_data.SOC.copy()\n",
    "seed['SOC2'] = individual_data.SOC2.copy()\n",
    "seed['care'] = individual_data.PROVCARE.copy()\n",
    "seed['socio-economic-class'] = individual_data.NSSECB.copy()\n",
    "seed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.dropna(axis='index', how='any', inplace=True)\n",
    "assert not seed.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_types = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.first()\n",
    "household_sizes = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.count()\n",
    "mask_couples_children = household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN) & (household_sizes <= 2)]\n",
    "mask_couples_no_children = household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN) & (household_sizes != 2)]\n",
    "invalids = (\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN) & (household_sizes <= 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN) & (household_sizes != 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.LONE_PARENT_WITH_DEPENDENT_CHILDREN) & (household_sizes < 2)] |\n",
    "    household_sizes[(household_types == ktp.census.HouseholdType.MULTI_PERSON_HOUSEHOLD) & (household_sizes <= 2)]\n",
    ")\n",
    "\n",
    "seed.drop(labels=invalids.index, level=None, inplace=True)\n",
    "\n",
    "\n",
    "print(\"{} households are invalid and were removed.\".format(invalids.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not ((seed.economicActivity == ktp.census.EconomicActivity.ABOVE_74) & (seed.age <= 74)).any()\n",
    "assert not ((seed.economicActivity == ktp.census.EconomicActivity.BELOW_16) & (seed.age >= 16)).any()\n",
    "assert not ((seed.economicActivity == ktp.census.EconomicActivity.BELOW_16) & (seed.qualification != ktp.census.Qualification.BELOW_16)).any()\n",
    "assert not ((seed.economicActivity != ktp.census.EconomicActivity.BELOW_16) & (seed.qualification == ktp.census.Qualification.BELOW_16)).any()\n",
    "\n",
    "household_types = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.first()\n",
    "household_sizes = seed.groupby((seed.index.get_level_values(0), seed.index.get_level_values(1))).hhtype.count()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.ONE_PERSON_HOUSEHOLD] == 1).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.COUPLE_WITH_DEPENDENT_CHILDREN] > 2).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.LONE_PARENT_WITH_DEPENDENT_CHILDREN] > 1).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.COUPLE_WITHOUT_DEPENDENT_CHILDREN] == 2).all()\n",
    "assert (household_sizes[household_types == ktp.census.HouseholdType.MULTI_PERSON_HOUSEHOLD] > 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_data = read_diary_file(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')\n",
    "diary_data_ts = read_diary_file_as_timeseries(TUS_DATA_FOLDER_PATH / 'tab' / 'diary_data_8.tab')[['activity', 'location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ts = pd.DataFrame({\n",
    "    'location': diary_data_ts.location.map(ktp.tus.LOCATION_MAP),\n",
    "    'activity': diary_data_ts.activity.map(ktp.tus.ACTIVITY_MAP)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ts.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simple_ts[(simple_ts.activity == ktp.tus.Activity.UNKNOWN) | (simple_ts.location == ktp.tus.Location.UNKNOWN)]) / len(simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5% of all entries are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts = simple_ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.replace(to_replace=[ktp.tus.Location.UNKNOWN, ktp.tus.Activity.UNKNOWN], value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknowns will be filled by forward fill. That is, whenever  an acticity/location is unknown it is expected that the last known activity/location is still valid. \n",
    "\n",
    "When doing that, it is important to not forward fill between diaries (all diaries are below each other). Hence, they must be grouped into diaries first and then forward filled. This will lead to the fact that not all Unknowns can be filled (the ones at the beginning of the day), but that is wanted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts = filled_simple_ts.groupby([filled_simple_ts.index.get_level_values(0), \n",
    "                                             filled_simple_ts.index.get_level_values(1), \n",
    "                                             filled_simple_ts.index.get_level_values(2), \n",
    "                                             filled_simple_ts.index.get_level_values(3)]).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_simple_ts.isnull().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO don't forward fill over too long durations, e.g. not more than 1-2h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining nans are filtered in the Filter section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map to markov states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = ktp.tus.from_simplified_location_and_activity_to_people_model(filled_simple_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nan(markov_ts, diary_data):\n",
    "    \"\"\"Remove all diaries with at least one NaN.\"\"\"\n",
    "    nan_mask = markov_ts.groupby(by=lambda index: (index[0], index[1], index[2], index[3])).apply(lambda values: values.isnull().any())\n",
    "    return pd.DataFrame(markov_ts)[markov_ts.index.droplevel(4).isin(nan_mask[~nan_mask].index)]\n",
    "\n",
    "def remove_people_not_in_seed(markov_ts, seed):\n",
    "    mask = markov_ts.index.droplevel([3, 4]).isin(seed.index)\n",
    "    return markov_ts[mask]\n",
    "\n",
    "\n",
    "def filter_all_people_for_which_less_than_two_diaries_exist(markov_ts):\n",
    "    valid_mask = markov_ts.groupby([markov_ts.index.get_level_values(0), \n",
    "                                    markov_ts.index.get_level_values(1), \n",
    "                                    markov_ts.index.get_level_values(2)]).apply(lambda values: len(values) == 24 * 6 * 2)\n",
    "    return markov_ts[markov_ts.index.droplevel([3, 4]).isin(valid_mask[valid_mask].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = filter_nan(markov_ts, diary_data)\n",
    "assert not markov_ts.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = remove_people_not_in_seed(markov_ts, seed)\n",
    "markov_ts = filter_all_people_for_which_less_than_two_diaries_exist(markov_ts)\n",
    "seed = seed[seed.index.isin(markov_ts.index.droplevel([3, 4]))]\n",
    "assert len(seed.index) * 2 * 24 * 6 == len(markov_ts.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add DayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = diary_data[diary_data.DDAYW2 == diary.DDAYW2.WEEKDAY_MON___FRI]\n",
    "markov_ts['daytype'] = 'weekend'\n",
    "markov_ts.loc[markov_ts.index.droplevel(4).isin(weekdays.index), 'daytype'] = 'weekday'\n",
    "markov_ts = markov_ts.reset_index(level=[3, 4]).set_index(['daytype', 'time_of_day'], append=True)\n",
    "markov_ts.drop('SN4', axis=1, inplace=True)\n",
    "markov_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts = markov_ts.unstack([0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from http://stackoverflow.com/a/39266194/1856079\n",
    "import scipy.stats as ss\n",
    "\n",
    "def cramers_corrected_stat(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correlation = pd.Series(\n",
    "    index=combinations(all_features, 2),\n",
    "    data=[cramers_corrected_stat(pd.crosstab(seed[feature1], seed[feature2]))\n",
    "          for feature1, feature2 in combinations(all_features, 2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = feature_correlation.sort_values().plot.barh(figsize=(14, 7))\n",
    "_ = plt.title('Cramer phi estimation of people feature association')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'people-feature-association.png').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One dimenional features\n",
    "\n",
    "First, let's look at one dimensional features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ts.ix['weekend', 10].map(lambda x: x.value).plot()\n",
    "_ = plt.yticks([1, 2, 3, 4, 5], [x for x in ppl.Activity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_function(x, y):\n",
    "    # cantor pairing function, http://stackoverflow.com/a/919661/1856079\n",
    "    return int(1/2 * (x + y) * (x + y + 1) + y)\n",
    "\n",
    "\n",
    "def feature_id(feature_values):\n",
    "    # transform enums to ints\n",
    "    if not isinstance(feature_values, tuple) and not isinstance(feature_values, pd.Series): # single feature value\n",
    "        if isinstance(feature_values, Enum):\n",
    "            feature_values = int(feature_values.value)\n",
    "    else:\n",
    "        if isinstance(feature_values[0], Enum):\n",
    "            feature_values = tuple(int(feature_value.value) for feature_value in feature_values)\n",
    "    \n",
    "    # calculate id\n",
    "    if not isinstance(feature_values, tuple) and not isinstance(feature_values, pd.Series): # single feature value\n",
    "        return feature_values\n",
    "    elif len(feature_values) == 2:\n",
    "        return pairing_function(feature_values[0], feature_values[1])\n",
    "    else:\n",
    "        return pairing_function(feature_id(feature_values[:-1]), feature_values[-1])\n",
    "\n",
    "        \n",
    "def cramers_phi_for_feature(feature):\n",
    "    if isinstance(feature, pd.Series): # 1D\n",
    "        feature_ids = feature.apply(feature_id)\n",
    "    elif isinstance(feature, pd.DataFrame): # 2D or more\n",
    "        feature_ids = feature.apply(feature_id, axis=1)\n",
    "    else:\n",
    "        raise ValueError('Feature must be pandas series or dataframe.')\n",
    "    def cramers_phi(series):\n",
    "        return cramers_corrected_stat(pd.crosstab(series.values, feature_ids))\n",
    "    return cramers_phi\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class TestFeature(Enum):\n",
    "    A = 1\n",
    "    B = 2\n",
    "    C = 3\n",
    "\n",
    "assert feature_id(TestFeature.A) == 1\n",
    "assert feature_id((TestFeature.A, TestFeature.B)) == pairing_function(1, 2)\n",
    "assert feature_id(pd.Series([TestFeature.A, TestFeature.B])) == pairing_function(1, 2)\n",
    "assert feature_id((TestFeature.A, TestFeature.B, TestFeature.C)) == pairing_function(pairing_function(1, 2), 3)\n",
    "assert feature_id(pd.Series([TestFeature.A, TestFeature.B, TestFeature.C])) == pairing_function(pairing_function(1, 2), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_corr_1d = pd.DataFrame({\n",
    "    'age': markov_ts.apply(cramers_phi_for_feature(seed.ageGroup), axis=1),\n",
    "    'income': markov_ts.apply(cramers_phi_for_feature(seed.personalIncome), axis=1),\n",
    "    'care': markov_ts.apply(cramers_phi_for_feature(seed.care), axis=1),\n",
    "    'qualification': markov_ts.apply(cramers_phi_for_feature(seed.qualification), axis=1),\n",
    "    'economicActivity': markov_ts.apply(cramers_phi_for_feature(seed.economicActivity), axis=1),\n",
    "    'hhtype': markov_ts.apply(cramers_phi_for_feature(seed.hhtype), axis=1),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ts_corr_1d.plot(figsize=(14, 7))\n",
    "_ = plt.title(\"Cramer's phi for each time step of people model\")\n",
    "_ = plt.ylabel(\"Cramer's phi\")\n",
    "_ = plt.xlabel(\"time step\")\n",
    "fig = ax.get_figure()\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'markov_ts_cramer_1d.png').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_corr_1d.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_corr_2d = pd.DataFrame({\n",
    "    feature_combination: markov_ts.apply(cramers_phi_for_feature(seed[[feature_combination[0], feature_combination[1]]]), axis=1)\n",
    "    for feature_combination in combinations(all_features, 2)\n",
    "})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_1d = ts_corr_1d.loc[:, ts_corr_1d.mean() == ts_corr_1d.mean().max()]\n",
    "ax = ts_corr_2d.loc[:, ts_corr_2d.mean() > float(best_1d.mean())].plot(figsize=(14, 7))\n",
    "_ = plt.title(\"Cramer's phi for each time step of people model\")\n",
    "_ = plt.ylabel(\"Cramer's phi\")\n",
    "_ = plt.xlabel(\"time step\")\n",
    "fig = ax.get_figure()\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'markov_ts_cramer_2d.png').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_corr_2d.loc[:, ts_corr_2d.mean() > ts_corr_1d.mean().max()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_corr_3d = pd.DataFrame({\n",
    "    (feature1, feature2, feature3): markov_ts.apply(cramers_phi_for_feature(seed[[feature1, feature2, feature3]]), axis=1)\n",
    "    for feature1, feature2, feature3 in combinations(all_features, 3)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ts_corr_3d.loc[:, ts_corr_3d.mean() > ts_corr_2d.mean().max()].plot(figsize=(14, 7))\n",
    "_ = plt.title(\"Cramer's phi for each time step of people model\")\n",
    "_ = plt.ylabel(\"Cramer's phi\")\n",
    "_ = plt.xlabel(\"time step\")\n",
    "fig = ax.get_figure()\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'markov_ts_cramer_3d.png').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_corr_3d.loc[:, ts_corr_3d.mean() > ts_corr_2d.mean().max()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Inspection of Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "GREY_COLORMAP = ListedColormap(sns.light_palette(\"black\", 30)[2:20])\n",
    "GAUSSIAN_SIGMA = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_markov_ts = markov_ts.copy()\n",
    "color_markov_ts.replace(to_replace=ppl.Activity.NOT_AT_HOME, value=0, inplace=True)\n",
    "color_markov_ts.replace(to_replace=ppl.Activity.SLEEP_AT_OTHER_HOME, value=0, inplace=True)\n",
    "color_markov_ts.replace(to_replace=ppl.Activity.OTHER_HOME, value=0, inplace=True)\n",
    "color_markov_ts.replace(to_replace=ppl.Activity.SLEEP_AT_HOME, value=0.5, inplace=True)\n",
    "color_markov_ts.replace(to_replace=ppl.Activity.HOME, value=1.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(gaussian_filter(color_markov_ts, sigma=GAUSSIAN_SIGMA), cmap=GREY_COLORMAP, cbar=False)\n",
    "_ = plt.xticks([])\n",
    "_ = plt.yticks([])\n",
    "_ = plt.xlabel('people')\n",
    "_ = plt.ylabel('time of day')\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'all-diaries-original.png').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustered by economic activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_seed = seed.sort_values(by='economicActivity')\n",
    "tranposed_markov_ts = color_markov_ts.transpose()\n",
    "tranposed_markov_ts.index = tranposed_markov_ts.index.droplevel(0)\n",
    "last_entries_in_group = sorted_seed.reset_index().groupby('economicActivity').last()[['SN1', 'SN2', 'SN3']]\n",
    "cluster_boundaries = [sorted_seed.reset_index()[(sorted_seed.reset_index().SN1 == last_entries_in_group.iloc[i, 0]) &\n",
    "                                                (sorted_seed.reset_index().SN2 == last_entries_in_group.iloc[i, 1]) &\n",
    "                                                (sorted_seed.reset_index().SN3 == last_entries_in_group.iloc[i, 2])].index.values[0]\n",
    "                      for i in range(len(last_entries_in_group))]\n",
    "cluster_boundaries = pd.Series(cluster_boundaries)\n",
    "label_locations = cluster_boundaries.shift().fillna(0) + cluster_boundaries.diff().fillna(cluster_boundaries[0]) / 2\n",
    "label_locations = label_locations.astype(np.int16)\n",
    "sorted_markov_ts = tranposed_markov_ts.reindex(sorted_index).dropna().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(gaussian_filter(sorted_markov_ts, sigma=GAUSSIAN_SIGMA), cmap=GREY_COLORMAP, cbar=False)\n",
    "plt.vlines(cluster_boundaries, ymin=0, ymax=288, color='black', linewidth=2)\n",
    "_ = plt.xticks(label_locations.values, sorted_seed.economicActivity.iloc[cluster_boundaries].values)\n",
    "_ = plt.yticks([])\n",
    "_ = plt.xlabel('people')\n",
    "_ = plt.ylabel('time of day')\n",
    "fig.autofmt_xdate()\n",
    "fig.savefig((BUILD_FOLDER_PATH / 'all-diaries-economic-activity-cluster.png').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ktp-paper]",
   "language": "python",
   "name": "conda-env-ktp-paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
